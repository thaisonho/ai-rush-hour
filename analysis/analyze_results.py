import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class ResultAnalyzer:
    def __init__(self, results_file='experiment_results.json'):
        """
        Kh·ªüi t·∫°o analyzer v·ªõi file k·∫øt qu·∫£
        
        Args:
            results_file: ƒê∆∞·ªùng d·∫´n t·ªõi file JSON ch·ª©a k·∫øt qu·∫£ th·ª≠ nghi·ªám
        """
        # X√°c ƒë·ªãnh th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ (c√πng th∆∞ m·ª•c v·ªõi file n√†y)
        self.output_dir = Path(__file__).parent
        
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n file k·∫øt qu·∫£
        if not Path(results_file).is_absolute():
            # N·∫øu l√† ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi, t√¨m t·ª´ th∆∞ m·ª•c hi·ªán t·∫°i
            current_dir = Path.cwd()
            possible_paths = [
                current_dir / results_file,
                current_dir / 'src' / results_file,
                current_dir / 'src' / 'experiment_results' / 'experiment_results.json',
                Path(__file__).parent / results_file,
                Path(__file__).parent / 'experiment_results' / 'experiment_results.json'
            ]
            
            self.results_file = None
            for path in possible_paths:
                if path.exists():
                    self.results_file = path
                    break
            
            if self.results_file is None:
                print("Kh√¥ng t√¨m th·∫•y file k·∫øt qu·∫£. C√°c ƒë∆∞·ªùng d·∫´n ƒë√£ th·ª≠:")
                for path in possible_paths:
                    print(f"  - {path}")
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file k·∫øt qu·∫£ experiment_results.json")
        else:
            self.results_file = Path(results_file)
        
        print(f"S·ª≠ d·ª•ng file k·∫øt qu·∫£: {self.results_file}")
        print(f"K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {self.output_dir}")
        self.data = None
        self.df = None
        self.df_solved = None  # Ch·ªâ c√°c k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i
        
    def load_data(self):
        """Load d·ªØ li·ªáu t·ª´ file JSON"""
        if not self.results_file.exists():
            raise FileNotFoundError(f"File {self.results_file} kh√¥ng t·ªìn t·∫°i!")
        
        with open(self.results_file, 'r') as f:
            self.data = json.load(f)
        
        # Chuy·ªÉn ƒë·ªïi th√†nh DataFrame
        self.df = pd.DataFrame(self.data)
        
        # Ch·ªâ l·∫•y c√°c k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i v√† th√†nh c√¥ng
        self.df_solved = self.df[
            (self.df['solution_found'] == True) & 
            (self.df['success'] == True)
        ].copy()
        
        print(f"ƒê√£ load {len(self.df)} k·∫øt qu·∫£ th·ª≠ nghi·ªám")
        print(f"Trong ƒë√≥ {len(self.df_solved)} k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i")
        
    def analyze_success_rates(self):
        """Ph√¢n t√≠ch t·ª∑ l·ªá th√†nh c√¥ng c·ªßa c√°c thu·∫≠t to√°n"""
        print("\n" + "="*60)
        print("PH√ÇN T√çCH T·ª∂ L·ªÜ TH√ÄNH C√îNG")
        print("="*60)
        
        # T·ªïng quan
        total_experiments = len(self.df)
        successful_experiments = len(self.df[self.df['success'] == True])
        solved_experiments = len(self.df_solved)
        
        print(f"T·ªïng s·ªë th·ª≠ nghi·ªám: {total_experiments}")
        print(f"Th·ª≠ nghi·ªám th√†nh c√¥ng: {successful_experiments} ({successful_experiments/total_experiments*100:.1f}%)")
        print(f"Th·ª≠ nghi·ªám t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i: {solved_experiments} ({solved_experiments/total_experiments*100:.1f}%)")
        
        # Ph√¢n t√≠ch theo thu·∫≠t to√°n
        print("\nT·ª∑ l·ªá th√†nh c√¥ng theo thu·∫≠t to√°n:")
        success_stats = self.df.groupby('algorithm').agg({
            'success': ['count', 'sum'],
            'solution_found': 'sum'
        }).round(2)
        
        success_stats.columns = ['total_runs', 'successful_runs', 'solutions_found']
        success_stats['success_rate'] = (success_stats['successful_runs'] / success_stats['total_runs'] * 100).round(1)
        success_stats['solution_rate'] = (success_stats['solutions_found'] / success_stats['total_runs'] * 100).round(1)
        
        print(success_stats[['total_runs', 'successful_runs', 'solutions_found', 'success_rate', 'solution_rate']])
        
        # Ph√¢n t√≠ch theo map
        print("\nT·ª∑ l·ªá th√†nh c√¥ng theo map:")
        map_stats = self.df.groupby('map').agg({
            'success': ['count', 'sum'],
            'solution_found': 'sum'
        }).round(2)
        
        map_stats.columns = ['total_runs', 'successful_runs', 'solutions_found']
        map_stats['success_rate'] = (map_stats['successful_runs'] / map_stats['total_runs'] * 100).round(1)
        map_stats['solution_rate'] = (map_stats['solutions_found'] / map_stats['total_runs'] * 100).round(1)
        
        print(map_stats[['total_runs', 'successful_runs', 'solutions_found', 'success_rate', 'solution_rate']])
        
        return success_stats, map_stats
    
    def analyze_performance_for_solved(self):
        """Ph√¢n t√≠ch hi·ªáu su·∫•t CH·ªà cho c√°c thu·∫≠t to√°n t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i"""
        if self.df_solved.empty:
            print("Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i ƒë·ªÉ ph√¢n t√≠ch!")
            return
            
        print("\n" + "="*60)
        print("PH√ÇN T√çCH HI·ªÜU SU·∫§T (CH·ªà C√ÅC THU·∫¨T TO√ÅN T√åM ƒê∆Ø·ª¢C L·ªúI GI·∫¢I)")
        print("="*60)
        
        # Th·ªëng k√™ t·ªïng quan
        performance_stats = self.df_solved.groupby('algorithm').agg({
            'search_time': ['mean', 'std', 'min', 'max'],
            'memory_usage': ['mean', 'std', 'min', 'max'],
            'nodes_expanded': ['mean', 'std', 'min', 'max'],
            'solution_length': ['mean', 'std', 'min', 'max']
        }).round(3)
        
        print("\n1. TH·ªúI GIAN T√åM KI·∫æM (gi√¢y):")
        print(performance_stats['search_time'])
        
        print("\n2. S·ª¨ D·ª§NG B·ªò NH·ªö (KB):")
        print(performance_stats['memory_usage'])
        
        print("\n3. S·ªê NODE ƒê∆Ø·ª¢C M·ªû R·ªòNG:")
        print(performance_stats['nodes_expanded'])
        
        print("\n4. ƒê·ªò D√ÄI L·ªúI GI·∫¢I:")
        print(performance_stats['solution_length'])
        
        # Ranking thu·∫≠t to√°n
        print("\n" + "="*60)
        print("RANKING THU·∫¨T TO√ÅN (CH·ªà T√çNH C√ÅC THU·∫¨T TO√ÅN T√åM ƒê∆Ø·ª¢C L·ªúI GI·∫¢I)")
        print("="*60)
        
        avg_stats = self.df_solved.groupby('algorithm').agg({
            'search_time': 'mean',
            'memory_usage': 'mean',
            'nodes_expanded': 'mean',
            'solution_length': 'mean'
        }).round(3)
        
        print("\nThu·∫≠t to√°n NHANH NH·∫§T (th·ªùi gian t√¨m ki·∫øm th·∫•p nh·∫•t):")
        fastest = avg_stats.sort_values('search_time')
        for i, (alg, row) in enumerate(fastest.iterrows(), 1):
            print(f"{i}. {alg}: {row['search_time']:.3f}s")
        
        print("\nThu·∫≠t to√°n TI·∫æT KI·ªÜM B·ªò NH·ªö NH·∫§T:")
        memory_efficient = avg_stats.sort_values('memory_usage')
        for i, (alg, row) in enumerate(memory_efficient.iterrows(), 1):
            print(f"{i}. {alg}: {row['memory_usage']:.1f}KB")
        
        print("\nThu·∫≠t to√°n HI·ªÜU QU·∫¢ NH·∫§T (√≠t node m·ªü r·ªông nh·∫•t):")
        node_efficient = avg_stats.sort_values('nodes_expanded')
        for i, (alg, row) in enumerate(node_efficient.iterrows(), 1):
            print(f"{i}. {alg}: {row['nodes_expanded']:.0f} nodes")
        
        print("\nThu·∫≠t to√°n T√åM L·ªúI GI·∫¢I T·ªêI ∆ØU NH·∫§T (l·ªùi gi·∫£i ng·∫Øn nh·∫•t):")
        optimal = avg_stats.sort_values('solution_length')
        for i, (alg, row) in enumerate(optimal.iterrows(), 1):
            print(f"{i}. {alg}: {row['solution_length']:.1f} moves")
        
        return performance_stats, avg_stats
    
    def analyze_map_difficulty(self):
        """Ph√¢n t√≠ch ƒë·ªô kh√≥ c·ªßa c√°c map"""
        print("\n" + "="*60)
        print("PH√ÇN T√çCH ƒê·ªò KH√ì C·ª¶A C√ÅC MAP")
        print("="*60)
        
        # Ch·ªâ xem x√©t c√°c map c√≥ √≠t nh·∫•t 1 thu·∫≠t to√°n t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i
        map_difficulty = self.df_solved.groupby('map').agg({
            'search_time': ['mean', 'std'],
            'nodes_expanded': ['mean', 'std'],
            'solution_length': ['mean', 'min', 'max']
        }).round(2)
        
        print("\nƒê·ªô kh√≥ theo s·ªë node trung b√¨nh c·∫ßn m·ªü r·ªông:")
        difficulty_ranking = map_difficulty.sort_values(('nodes_expanded', 'mean'))
        for i, (map_name, row) in enumerate(difficulty_ranking.iterrows(), 1):
            avg_nodes = row[('nodes_expanded', 'mean')]
            avg_time = row[('search_time', 'mean')]
            print(f"{i}. {map_name}: {avg_nodes:.0f} nodes, {avg_time:.3f}s")
        
        # Ph√¢n t√≠ch maps kh√¥ng c√≥ l·ªùi gi·∫£i
        unsolved_maps = self.df[self.df['solution_found'] == False]['map'].unique()
        if len(unsolved_maps) > 0:
            print(f"\nC√°c map c√≥ thu·∫≠t to√°n kh√¥ng t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i:")
            for map_name in unsolved_maps:
                unsolved_algs = self.df[
                    (self.df['map'] == map_name) & 
                    (self.df['solution_found'] == False)
                ]['algorithm'].unique()
                print(f"  {map_name}: {', '.join(unsolved_algs)}")
        
        return map_difficulty
    
    def compare_algorithms_by_map(self):
        """So s√°nh hi·ªáu su·∫•t thu·∫≠t to√°n theo t·ª´ng map"""
        print("\n" + "="*60)
        print("SO S√ÅNH THU·∫¨T TO√ÅN THEO T·ª™NG MAP")
        print("="*60)
        
        for map_name in sorted(self.df['map'].unique()):
            map_data = self.df_solved[self.df_solved['map'] == map_name]
            
            if map_data.empty:
                print(f"\n{map_name}: Kh√¥ng c√≥ thu·∫≠t to√°n n√†o t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i!")
                continue
                
            print(f"\n{map_name}:")
            print("-" * 40)
            
            # S·∫Øp x·∫øp theo th·ªùi gian t√¨m ki·∫øm
            map_stats = map_data.groupby('algorithm').agg({
                'search_time': 'mean',
                'memory_usage': 'mean',
                'nodes_expanded': 'mean',
                'solution_length': 'mean'
            }).round(3)
            
            sorted_by_time = map_stats.sort_values('search_time')
            
            print("Ranking theo th·ªùi gian:")
            for i, (alg, row) in enumerate(sorted_by_time.iterrows(), 1):
                print(f"  {i}. {alg}: {row['search_time']:.3f}s, "
                      f"{row['nodes_expanded']:.0f} nodes, "
                      f"{row['solution_length']:.0f} moves")
    
    def create_visualizations(self):
        """T·∫°o c√°c bi·ªÉu ƒë·ªì ph√¢n t√≠ch"""
        if self.df_solved.empty:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o bi·ªÉu ƒë·ªì!")
            return
        
        # Thi·∫øt l·∫≠p style
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # 1. Success Rate Comparison
        plt.figure(figsize=(12, 8))
        success_rate = self.df.groupby('algorithm')['solution_found'].mean() * 100
        bars = plt.bar(success_rate.index, success_rate.values, color='skyblue', edgecolor='navy')
        plt.title('Algorithm Success Rate Comparison', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Success Rate (%)', fontsize=12)
        plt.ylim(0, 100)
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{height:.1f}%', ha='center', va='bottom', fontsize=10)
        
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'success_rate_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Search Time Comparison
        plt.figure(figsize=(12, 8))
        perf_data = self.df_solved.groupby('algorithm')['search_time'].mean()
        bars = plt.bar(perf_data.index, perf_data.values, color='lightcoral', edgecolor='darkred')
        plt.title('Search Time Comparison (Only Solved Cases)', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Average Search Time (seconds)', fontsize=12)
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'search_time_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Memory Usage Comparison
        plt.figure(figsize=(12, 8))
        memory_data = self.df_solved.groupby('algorithm')['memory_usage'].mean()
        bars = plt.bar(memory_data.index, memory_data.values, color='lightgreen', edgecolor='darkgreen')
        plt.title('Memory Usage Comparison (Only Solved Cases)', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Average Memory Usage (KB)', fontsize=12)
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'memory_usage_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. Nodes Expanded Comparison
        plt.figure(figsize=(12, 8))
        nodes_data = self.df_solved.groupby('algorithm')['nodes_expanded'].mean()
        bars = plt.bar(nodes_data.index, nodes_data.values, color='gold', edgecolor='orange')
        plt.title('Nodes Expanded Comparison (Only Solved Cases)', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Average Nodes Expanded', fontsize=12)
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'nodes_expanded_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 5. Solution Quality Comparison
        plt.figure(figsize=(12, 8))
        quality_data = self.df_solved.groupby('algorithm')['solution_length'].mean()
        bars = plt.bar(quality_data.index, quality_data.values, color='mediumpurple', edgecolor='purple')
        plt.title('Solution Quality Comparison (Only Solved Cases)', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Average Solution Length (moves)', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'solution_quality_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 6. Performance Heatmap
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        metrics = ['search_time', 'memory_usage', 'nodes_expanded', 'solution_length']
        titles = ['Search Time', 'Memory Usage', 'Nodes Expanded', 'Solution Length']
        
        for i, (metric, title) in enumerate(zip(metrics, titles)):
            row, col = i // 2, i % 2
            
            pivot = self.df_solved.pivot_table(
                index='algorithm', 
                columns='map', 
                values=metric, 
                aggfunc='mean'
            )
            
            sns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', 
                       ax=axes[row, col], cbar_kws={'label': title})
            axes[row, col].set_title(f'{title} Heatmap', fontsize=14, fontweight='bold')
            axes[row, col].set_xlabel('Map', fontsize=12)
            axes[row, col].set_ylabel('Algorithm', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'performance_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 7. Performance Dashboard
        fig = plt.figure(figsize=(20, 15))
        
        # Create subplots
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # 1. Average performance by algorithm
        ax1 = fig.add_subplot(gs[0, :])
        avg_metrics = self.df_solved.groupby('algorithm')[['search_time', 'memory_usage', 'nodes_expanded']].mean()
        avg_metrics.plot(kind='bar', ax=ax1, log=True)
        ax1.set_title('Average Performance Metrics by Algorithm', fontsize=16, fontweight='bold')
        ax1.set_ylabel('Value (log scale)')
        ax1.legend(['Search Time (s)', 'Memory Usage (KB)', 'Nodes Expanded'])
        
        # 2. Performance vs Map Difficulty
        ax2 = fig.add_subplot(gs[1, 0])
        map_difficulty = self.df_solved.groupby('map')['nodes_expanded'].mean().sort_values()
        ax2.plot(range(len(map_difficulty)), map_difficulty.values, 'o-')
        ax2.set_title('Map Difficulty\n(by avg nodes expanded)', fontsize=12)
        ax2.set_xlabel('Map (sorted by difficulty)')
        ax2.set_ylabel('Avg Nodes Expanded')
        ax2.set_xticks(range(len(map_difficulty)))
        ax2.set_xticklabels(map_difficulty.index, rotation=45)
        
        # 3. Algorithm efficiency (Time vs Nodes)
        ax3 = fig.add_subplot(gs[1, 1])
        for alg in self.df_solved['algorithm'].unique():
            alg_data = self.df_solved[self.df_solved['algorithm'] == alg]
            ax3.scatter(alg_data['nodes_expanded'], alg_data['search_time'], 
                       label=alg, alpha=0.7, s=50)
        ax3.set_xlabel('Nodes Expanded')
        ax3.set_ylabel('Search Time (s)')
        ax3.set_title('Algorithm Efficiency\n(Time vs Nodes)', fontsize=12)
        ax3.legend()
        ax3.set_xscale('log')
        ax3.set_yscale('log')
        
        # 4. Memory efficiency
        ax4 = fig.add_subplot(gs[1, 2])
        for alg in self.df_solved['algorithm'].unique():
            alg_data = self.df_solved[self.df_solved['algorithm'] == alg]
            ax4.scatter(alg_data['nodes_expanded'], alg_data['memory_usage'], 
                       label=alg, alpha=0.7, s=50)
        ax4.set_xlabel('Nodes Expanded')
        ax4.set_ylabel('Memory Usage (KB)')
        ax4.set_title('Memory Efficiency', fontsize=12)
        ax4.legend()
        ax4.set_xscale('log')
        
        # 5. Solution quality distribution
        ax5 = fig.add_subplot(gs[2, :])
        self.df_solved.boxplot(column='solution_length', by='algorithm', ax=ax5)
        ax5.set_title('Solution Quality Distribution by Algorithm', fontsize=14)
        ax5.set_xlabel('Algorithm')
        ax5.set_ylabel('Solution Length (moves)')
        
        plt.suptitle('AI Rush Hour - Algorithm Performance Dashboard', fontsize=20, fontweight='bold')
        plt.savefig(self.output_dir / 'performance_dashboard.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ƒê√£ t·∫°o c√°c file bi·ªÉu ƒë·ªì:")
        print(f"  - {self.output_dir / 'success_rate_comparison.png'}")
        print(f"  - {self.output_dir / 'search_time_comparison.png'}")
        print(f"  - {self.output_dir / 'memory_usage_comparison.png'}")
        print(f"  - {self.output_dir / 'nodes_expanded_comparison.png'}")
        print(f"  - {self.output_dir / 'solution_quality_comparison.png'}")
        print(f"  - {self.output_dir / 'performance_heatmap.png'}")
        print(f"  - {self.output_dir / 'performance_dashboard.png'}")
    
    def generate_summary_report(self):
        """T·∫°o b√°o c√°o t·ªïng k·∫øt"""
        print("\n" + "="*80)
        print("B√ÅO C√ÅO T·ªîNG K·∫æT - PH√ÇN T√çCH CH√çNH X√ÅC")
        print("="*80)
        
        # T·ªïng quan
        total_maps = self.df['map'].nunique()
        total_algorithms = self.df['algorithm'].nunique()
        
        print(f"T·ªïng s·ªë map ƒë∆∞·ª£c test: {total_maps}")
        print(f"T·ªïng s·ªë thu·∫≠t to√°n: {total_algorithms}")
        print(f"T·ªïng s·ªë th·ª≠ nghi·ªám: {len(self.df)}")
        print(f"S·ªë th·ª≠ nghi·ªám t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i: {len(self.df_solved)}")
        
        # Thu·∫≠t to√°n t·ªët nh·∫•t
        if not self.df_solved.empty:
            avg_stats = self.df_solved.groupby('algorithm').agg({
                'search_time': 'mean',
                'memory_usage': 'mean',
                'nodes_expanded': 'mean',
                'solution_length': 'mean'
            })
            
            print("\nTHU·∫¨T TO√ÅN T·ªêT NH·∫§T (d·ª±a tr√™n c√°c thu·∫≠t to√°n t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i):")
            print(f"  Nhanh nh·∫•t: {avg_stats['search_time'].idxmin()}")
            print(f"  Ti·∫øt ki·ªám b·ªô nh·ªõ nh·∫•t: {avg_stats['memory_usage'].idxmin()}")
            print(f"  Hi·ªáu qu·∫£ nh·∫•t (√≠t node nh·∫•t): {avg_stats['nodes_expanded'].idxmin()}")
            print(f"  T·ªëi ∆∞u nh·∫•t (l·ªùi gi·∫£i ng·∫Øn nh·∫•t): {avg_stats['solution_length'].idxmin()}")
        
        # T·ª∑ l·ªá th√†nh c√¥ng
        success_rate = self.df.groupby('algorithm')['solution_found'].mean() * 100
        print(f"\nThu·∫≠t to√°n ƒë√°ng tin c·∫≠y nh·∫•t: {success_rate.idxmax()} ({success_rate.max():.1f}%)")
        
        # Map kh√≥ nh·∫•t
        if not self.df_solved.empty:
            map_difficulty = self.df_solved.groupby('map')['nodes_expanded'].mean()
            print(f"\nMap d·ªÖ nh·∫•t: {map_difficulty.idxmin()} ({map_difficulty.min():.0f} nodes)")
            print(f"Map kh√≥ nh·∫•t: {map_difficulty.idxmax()} ({map_difficulty.max():.0f} nodes)")
        
        print("\n" + "="*80)
        print("K·∫æT LU·∫¨N:")
        print("- Ph√¢n t√≠ch n√†y ch·ªâ xem x√©t hi·ªáu su·∫•t c·ªßa c√°c thu·∫≠t to√°n KHI T√åM ƒê∆Ø·ª¢C L·ªúI GI·∫¢I")
        print("- C√°c thu·∫≠t to√°n kh√¥ng t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i ƒë∆∞·ª£c xem x√©t ri√™ng trong t·ª∑ l·ªá th√†nh c√¥ng")
        print("- ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c trong vi·ªác ƒë√°nh gi√° hi·ªáu su·∫•t thu·∫≠t to√°n")
        print("="*80)
    
    def get_common_solved_maps(self):
        """T√¨m c√°c map m√† T·∫§T C·∫¢ thu·∫≠t to√°n ƒë·ªÅu t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i"""
        all_algorithms = self.df['algorithm'].unique()
        common_maps = set()
        
        for map_name in self.df['map'].unique():
            map_data = self.df[self.df['map'] == map_name]
            solved_algorithms = set(map_data[map_data['solution_found'] == True]['algorithm'].unique())
            
            if len(solved_algorithms) == len(all_algorithms):
                common_maps.add(map_name)
        
        return common_maps

    def analyze_fair_comparison(self):
        """Ph√¢n t√≠ch so s√°nh c√¥ng b·∫±ng gi·ªØa c√°c thu·∫≠t to√°n"""
        print("\n" + "="*80)
        print("PH√ÇN T√çCH SO S√ÅNH C√îNG B·∫∞NG")
        print("="*80)
        
        all_algorithms = self.df['algorithm'].unique()
        total_maps = self.df['map'].nunique()
        
        # 1. T√¨m maps m√† t·∫•t c·∫£ thu·∫≠t to√°n ƒë·ªÅu t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i
        common_solved_maps = self.get_common_solved_maps()
        
        print(f"T·ªïng s·ªë maps: {total_maps}")
        print(f"Maps m√† T·∫§T C·∫¢ thu·∫≠t to√°n ƒë·ªÅu t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i: {len(common_solved_maps)}")
        
        if common_solved_maps:
            print(f"Danh s√°ch maps chung: {sorted(common_solved_maps)}")
            
            # Ph√¢n t√≠ch tr√™n t·∫≠p d·ªØ li·ªáu c√¥ng b·∫±ng
            fair_data = self.df[
                (self.df['map'].isin(common_solved_maps)) & 
                (self.df['solution_found'] == True)
            ]
            
            print("\n1. SO S√ÅNH HI·ªÜU SU·∫§T C√îNG B·∫∞NG (ch·ªâ maps m√† t·∫•t c·∫£ thu·∫≠t to√°n ƒë·ªÅu gi·∫£i ƒë∆∞·ª£c):")
            print("-" * 70)
            
            fair_stats = fair_data.groupby('algorithm').agg({
                'search_time': ['mean', 'std'],
                'memory_usage': ['mean', 'std'],
                'nodes_expanded': ['mean', 'std'],
                'solution_length': ['mean', 'std']
            }).round(3)
            
            print("\nTh·ªùi gian t√¨m ki·∫øm (gi√¢y):")
            time_ranking = fair_stats['search_time'].sort_values('mean')
            for i, (alg, row) in enumerate(time_ranking.iterrows(), 1):
                print(f"  {i}. {alg}: {row['mean']:.3f}s (¬±{row['std']:.3f})")
            
            print("\nS·ª≠ d·ª•ng b·ªô nh·ªõ (KB):")
            memory_ranking = fair_stats['memory_usage'].sort_values('mean')
            for i, (alg, row) in enumerate(memory_ranking.iterrows(), 1):
                print(f"  {i}. {alg}: {row['mean']:.1f}KB (¬±{row['std']:.1f})")
            
            print("\nS·ªë node m·ªü r·ªông:")
            nodes_ranking = fair_stats['nodes_expanded'].sort_values('mean')
            for i, (alg, row) in enumerate(nodes_ranking.iterrows(), 1):
                print(f"  {i}. {alg}: {row['mean']:.0f} nodes (¬±{row['std']:.0f})")
            
            print("\nƒê·ªô d√†i l·ªùi gi·∫£i:")
            solution_ranking = fair_stats['solution_length'].sort_values('mean')
            for i, (alg, row) in enumerate(solution_ranking.iterrows(), 1):
                print(f"  {i}. {alg}: {row['mean']:.1f} moves (¬±{row['std']:.1f})")
        
        # 2. Ph√¢n t√≠ch overlap gi·ªØa c√°c thu·∫≠t to√°n
        print("\n2. PH√ÇN T√çCH OVERLAP - Maps m√† t·ª´ng c·∫∑p thu·∫≠t to√°n c√πng gi·∫£i ƒë∆∞·ª£c:")
        print("-" * 70)
        self.analyze_pairwise_overlap()
        
        # 3. Ma tr·∫≠n th√†nh c√¥ng
        print("\n3. MA TR·∫¨N TH√ÄNH C√îNG:")
        print("-" * 70)
        self.create_success_matrix()
        
        # 4. ƒêi·ªÉm s·ªë c√¥ng b·∫±ng
        print("\n4. H·ªÜ TH·ªêNG CH·∫§M ƒêI·ªÇM C√îNG B·∫∞NG:")
        print("-" * 70)
        self.calculate_fair_scores()
        
        return fair_data if common_solved_maps else None

    def analyze_pairwise_overlap(self):
        """Ph√¢n t√≠ch overlap gi·ªØa t·ª´ng c·∫∑p thu·∫≠t to√°n"""
        import itertools
        
        algorithms = self.df['algorithm'].unique()
        
        for alg1, alg2 in itertools.combinations(algorithms, 2):
            # T√¨m maps m√† c·∫£ hai thu·∫≠t to√°n ƒë·ªÅu gi·∫£i ƒë∆∞·ª£c
            alg1_solved = set(self.df[
                (self.df['algorithm'] == alg1) & 
                (self.df['solution_found'] == True)
            ]['map'].unique())
            
            alg2_solved = set(self.df[
                (self.df['algorithm'] == alg2) & 
                (self.df['solution_found'] == True)
            ]['map'].unique())
            
            common_maps = alg1_solved.intersection(alg2_solved)
            
            if common_maps:
                print(f"\n{alg1} vs {alg2}:")
                print(f"  - {alg1} gi·∫£i ƒë∆∞·ª£c: {len(alg1_solved)} maps")
                print(f"  - {alg2} gi·∫£i ƒë∆∞·ª£c: {len(alg2_solved)} maps")
                print(f"  - C√πng gi·∫£i ƒë∆∞·ª£c: {len(common_maps)} maps")
                
                # So s√°nh hi·ªáu su·∫•t tr√™n maps chung
                if len(common_maps) > 0:
                    common_data = self.df[
                        (self.df['map'].isin(common_maps)) & 
                        (self.df['algorithm'].isin([alg1, alg2])) &
                        (self.df['solution_found'] == True)
                    ]
                    
                    if not common_data.empty:
                        comparison = common_data.groupby('algorithm').agg({
                            'search_time': 'mean',
                            'nodes_expanded': 'mean',
                            'solution_length': 'mean'
                        }).round(3)
                        
                        print(f"  - Hi·ªáu su·∫•t tr√™n {len(common_maps)} maps chung:")
                        print(f"    {alg1}: {comparison.loc[alg1, 'search_time']:.3f}s, {comparison.loc[alg1, 'nodes_expanded']:.0f} nodes")
                        print(f"    {alg2}: {comparison.loc[alg2, 'search_time']:.3f}s, {comparison.loc[alg2, 'nodes_expanded']:.0f} nodes")

    def create_success_matrix(self):
        """T·∫°o ma tr·∫≠n th√†nh c√¥ng algorithm vs map"""
        # T·∫°o ma tr·∫≠n success
        success_matrix = pd.crosstab(
            self.df['algorithm'], 
            self.df['map'], 
            self.df['solution_found'], 
            aggfunc='max'  # N·∫øu c√≥ multiple runs, l·∫•y max
        ).fillna(0).astype(int)
        
        print("\nMa tr·∫≠n th√†nh c√¥ng (1=t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i, 0=kh√¥ng t√¨m ƒë∆∞·ª£c):")
        print(success_matrix.to_string())
        
        # Th·ªëng k√™ t·ªïng k·∫øt
        print(f"\nT·ªïng k·∫øt:")
        for alg in success_matrix.index:
            solved_count = success_matrix.loc[alg].sum()
            total_maps = len(success_matrix.columns)
            success_rate = solved_count / total_maps * 100
            print(f"  {alg}: {solved_count}/{total_maps} maps ({success_rate:.1f}%)")
        
        return success_matrix

    def calculate_fair_scores(self):
        """T√≠nh ƒëi·ªÉm s·ªë c√¥ng b·∫±ng cho c√°c thu·∫≠t to√°n"""
        algorithms = self.df['algorithm'].unique()
        maps = self.df['map'].unique()
        
        # T·∫°o ma tr·∫≠n ƒëi·ªÉm
        scores = pd.DataFrame(index=algorithms, columns=maps, dtype=float)
        
        for map_name in maps:
            map_data = self.df[self.df['map'] == map_name]
            
            # Ch·ªâ t√≠nh ƒëi·ªÉm cho c√°c thu·∫≠t to√°n t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i
            solved_data = map_data[map_data['solution_found'] == True]
            
            if not solved_data.empty:
                # Ranking theo th·ªùi gian (thu·∫≠t to√°n nhanh nh·∫•t = ƒëi·ªÉm cao nh·∫•t)
                time_ranking = solved_data.set_index('algorithm')['search_time'].rank(ascending=True)
                max_rank = len(time_ranking)
                
                # Chuy·ªÉn ƒë·ªïi rank th√†nh ƒëi·ªÉm (rank 1 = ƒëi·ªÉm cao nh·∫•t)
                for alg in time_ranking.index:
                    scores.loc[alg, map_name] = max_rank - time_ranking[alg] + 1
            
            # Thu·∫≠t to√°n kh√¥ng t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i = 0 ƒëi·ªÉm
            unsolved_algs = map_data[map_data['solution_found'] == False]['algorithm'].unique()
            for alg in unsolved_algs:
                scores.loc[alg, map_name] = 0
        
        # T√≠nh t·ªïng ƒëi·ªÉm
        total_scores = scores.sum(axis=1, skipna=True).sort_values(ascending=False)
        
        print("B·∫£ng x·∫øp h·∫°ng t·ªïng th·ªÉ (d·ª±a tr√™n ƒëi·ªÉm ranking):")
        for i, (alg, score) in enumerate(total_scores.items(), 1):
            solved_count = (scores.loc[alg] > 0).sum()
            total_maps = len(maps)
            avg_score = score / total_maps if total_maps > 0 else 0
            print(f"  {i}. {alg}: {score:.1f} ƒëi·ªÉm ({solved_count}/{total_maps} maps, avg: {avg_score:.2f})")
        
        return scores, total_scores

    def create_fair_comparison_plots(self):
        """T·∫°o bi·ªÉu ƒë·ªì so s√°nh c√¥ng b·∫±ng"""
        # 1. Success rate matrix heatmap
        plt.figure(figsize=(14, 8))
        
        # T·∫°o ma tr·∫≠n success rate
        success_matrix = pd.crosstab(
            self.df['algorithm'], 
            self.df['map'], 
            self.df['solution_found'], 
            aggfunc='max'
        ).fillna(0)
        
        sns.heatmap(success_matrix, annot=True, fmt='d', cmap='RdYlGn', 
                    center=0.5, vmin=0, vmax=1, cbar_kws={'label': 'Solution Found'})
        plt.title('Success Matrix: Algorithm vs Map', fontsize=16, fontweight='bold')
        plt.ylabel('Algorithm', fontsize=12)
        plt.xlabel('Map', fontsize=12)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'success_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Fair comparison n·∫øu c√≥ common maps
        common_maps = self.get_common_solved_maps()
        if common_maps:
            fair_data = self.df[
                (self.df['map'].isin(common_maps)) & 
                (self.df['solution_found'] == True)
            ]
            
            if not fair_data.empty:
                fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                
                metrics = ['search_time', 'memory_usage', 'nodes_expanded', 'solution_length']
                titles = ['Search Time (s)', 'Memory Usage (KB)', 'Nodes Expanded', 'Solution Length']
                colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral']
                
                for i, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
                    ax = axes[i//2, i%2]
                    
                    # Box plot cho fair comparison
                    fair_data.boxplot(column=metric, by='algorithm', ax=ax)
                    ax.set_title(f'{title} - Fair Comparison\n({len(common_maps)} common maps)', fontsize=12)
                    ax.set_xlabel('Algorithm')
                    ax.set_ylabel(title)
                    
                    # Th√™m mean values
                    means = fair_data.groupby('algorithm')[metric].mean()
                    for j, (alg, mean_val) in enumerate(means.items()):
                        ax.text(j+1, mean_val, f'{mean_val:.2f}', 
                               ha='center', va='bottom', fontweight='bold', color='red')
                
                plt.suptitle('Fair Performance Comparison\n(Only maps solved by ALL algorithms)', 
                            fontsize=16, fontweight='bold')
                plt.tight_layout()
                plt.savefig(self.output_dir / 'fair_comparison.png', dpi=300, bbox_inches='tight')
                plt.close()
        
        # 3. Success rate comparison
        plt.figure(figsize=(12, 6))
        
        success_rates = self.df.groupby('algorithm')['solution_found'].mean() * 100
        bars = plt.bar(success_rates.index, success_rates.values, 
                       color='skyblue', edgecolor='navy', alpha=0.7)
        
        plt.title('Algorithm Success Rate Comparison', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Success Rate (%)', fontsize=12)
        plt.ylim(0, 100)
        
        # Add value labels
        for bar, rate in zip(bars, success_rates.values):
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,
                    f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.grid(True, alpha=0.3, axis='y')
        plt.tight_layout()
        plt.savefig(self.output_dir / 'success_rate_detailed.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ƒê√£ t·∫°o bi·ªÉu ƒë·ªì so s√°nh c√¥ng b·∫±ng:")
        print(f"  - {self.output_dir / 'success_matrix.png'}")
        print(f"  - {self.output_dir / 'success_rate_detailed.png'}")
        if common_maps:
            print(f"  - {self.output_dir / 'fair_comparison.png'}")
    
    def create_per_map_visualization(self):
        """T·∫°o bi·ªÉu ƒë·ªì so s√°nh hi·ªáu su·∫•t t·ª´ng map"""
        print(f"\nƒêang t·∫°o bi·ªÉu ƒë·ªì ph√¢n t√≠ch t·ª´ng map...")
        
        maps = sorted(self.df['map'].unique())
        
        # 1. Heatmap performance matrix cho t·ª´ng metric
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        metrics = ['search_time', 'memory_usage', 'nodes_expanded', 'solution_length']
        titles = ['Search Time (s)', 'Memory Usage (KB)', 'Nodes Expanded', 'Solution Length']
        
        for i, (metric, title) in enumerate(zip(metrics, titles)):
            ax = axes[i//2, i%2]
            
            # T·∫°o pivot table
            pivot = self.df_solved.pivot_table(
                index='algorithm', 
                columns='map', 
                values=metric, 
                aggfunc='mean'
            )
            
            # T·∫°o heatmap
            sns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', 
                       ax=ax, cbar_kws={'label': title})
            ax.set_title(f'{title} by Algorithm and Map', fontsize=14, fontweight='bold')
            ax.set_xlabel('Map', fontsize=12)
            ax.set_ylabel('Algorithm', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'per_map_performance_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Theoretical compliance visualization
        fig, ax = plt.subplots(1, 1, figsize=(16, 10))
        
        # T·∫°o matrix th√†nh c√¥ng cho m·ªói map
        success_matrix = pd.crosstab(
            self.df['map'], 
            self.df['algorithm'], 
            self.df['solution_found'], 
            aggfunc='max'
        ).fillna(0)
        
        sns.heatmap(success_matrix, annot=True, fmt='d', cmap='RdYlGn', 
                   center=0.5, vmin=0, vmax=1, ax=ax)
        ax.set_title('Algorithm Success by Map\n(1=Success, 0=Failure)', fontsize=16, fontweight='bold')
        ax.set_xlabel('Algorithm', fontsize=12)
        ax.set_ylabel('Map', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'algorithm_success_by_map.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Performance comparison radar chart cho t·ª´ng map
        if len(maps) <= 6:  # Ch·ªâ t·∫°o n·∫øu kh√¥ng qu√° nhi·ªÅu maps
            fig, axes = plt.subplots(2, 3, figsize=(18, 12), subplot_kw=dict(projection='polar'))
            axes = axes.flatten()
            
            for i, map_name in enumerate(maps):
                if i >= 6:  # Gi·ªõi h·∫°n 6 maps
                    break
                    
                ax = axes[i]
                map_data = self.df_solved[self.df_solved['map'] == map_name]
                
                if not map_data.empty:
                    # Chu·∫©n b·ªã d·ªØ li·ªáu cho radar chart
                    algorithms = map_data['algorithm'].unique()
                    angles = np.linspace(0, 2*np.pi, len(algorithms), endpoint=False)
                    
                    # Metrics ƒë·ªÉ so s√°nh (normalize v·ªÅ 0-1)
                    metrics = ['search_time', 'memory_usage', 'nodes_expanded', 'solution_length']
                    
                    for metric in metrics:
                        values = []
                        for alg in algorithms:
                            alg_data = map_data[map_data['algorithm'] == alg]
                            values.append(alg_data[metric].mean())
                        
                        # Normalize (inverse cho performance metrics)
                        max_val = max(values)
                        if max_val > 0:
                            normalized = [(max_val - v) / max_val for v in values]
                        else:
                            normalized = values
                        
                        ax.plot(angles, normalized, 'o-', linewidth=2, label=metric)
                    
                    ax.set_xticks(angles)
                    ax.set_xticklabels(algorithms)
                    ax.set_title(f'{map_name}', fontsize=12, fontweight='bold')
                    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))
            
            plt.tight_layout()
            plt.savefig(self.output_dir / 'per_map_radar_charts.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. Time complexity analysis
        fig, ax = plt.subplots(1, 1, figsize=(14, 8))
        
        # So s√°nh th·ªùi gian vs s·ªë nodes cho m·ªói thu·∫≠t to√°n
        colors = plt.cm.tab10(np.linspace(0, 1, len(self.df_solved['algorithm'].unique())))
        
        for i, alg in enumerate(sorted(self.df_solved['algorithm'].unique())):
            alg_data = self.df_solved[self.df_solved['algorithm'] == alg]
            
            ax.scatter(alg_data['nodes_expanded'], alg_data['search_time'], 
                      label=alg, alpha=0.7, s=60, color=colors[i])
        
        ax.set_xlabel('Nodes Expanded', fontsize=12)
        ax.set_ylabel('Search Time (s)', fontsize=12)
        ax.set_title('Time Complexity Analysis\n(Time vs Nodes Expanded)', fontsize=14, fontweight='bold')
        ax.legend()
        ax.set_xscale('log')
        ax.set_yscale('log')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'time_complexity_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ƒê√£ t·∫°o bi·ªÉu ƒë·ªì ph√¢n t√≠ch t·ª´ng map:")
        print(f"  - {self.output_dir / 'per_map_performance_heatmap.png'}")
        print(f"  - {self.output_dir / 'algorithm_success_by_map.png'}")
        print(f"  - {self.output_dir / 'time_complexity_analysis.png'}")
        if len(maps) <= 6:
            print(f"  - {self.output_dir / 'per_map_radar_charts.png'}")
    
    def generate_comprehensive_report(self):
        """T·∫°o b√°o c√°o t·ªïng h·ª£p"""
        print("\n" + "="*80)
        print("B√ÅO C√ÅO T·ªîNG H·ª¢P")
        print("="*80)
        
        # 1. Executive Summary
        print("\n1. T√ìM T·∫ÆT ƒêI·ªÄU H√ÄNH:")
        print("-" * 40)
        
        total_experiments = len(self.df)
        total_maps = self.df['map'].nunique()
        total_algorithms = self.df['algorithm'].nunique()
        
        print(f"  - T·ªïng s·ªë th·ª≠ nghi·ªám: {total_experiments}")
        print(f"  - S·ªë maps: {total_maps}")
        print(f"  - S·ªë thu·∫≠t to√°n: {total_algorithms}")
        
        # Overall success rate
        overall_success = self.df['solution_found'].mean() * 100
        print(f"  - T·ª∑ l·ªá th√†nh c√¥ng t·ªïng th·ªÉ: {overall_success:.1f}%")
        
        # 2. Algorithm Ranking
        print("\n2. X·∫æP H·∫†NG THU·∫¨T TO√ÅN:")
        print("-" * 40)
        
        # T√≠nh composite score
        algorithm_scores = self.calculate_composite_scores()
        
        for i, (alg, score) in enumerate(algorithm_scores.items(), 1):
            print(f"  {i}. {alg}: {score:.2f} ƒëi·ªÉm")
        
        # 3. Recommendations
        print("\n3. KHUY·∫æN NGH·ªä:")
        print("-" * 40)
        
        self.generate_recommendations(algorithm_scores)

    def calculate_composite_scores(self):
        """T√≠nh ƒëi·ªÉm t·ªïng h·ª£p cho t·ª´ng thu·∫≠t to√°n"""
        algorithms = self.df['algorithm'].unique()
        composite_scores = {}
        
        for alg in algorithms:
            alg_data = self.df[self.df['algorithm'] == alg]
            
            # C√°c metric ch√≠nh
            coverage_rate = alg_data['solution_found'].mean() * 100
            
            # Ch·ªâ t√≠nh performance cho c√°c cases ƒë√£ gi·∫£i ƒë∆∞·ª£c
            solved_data = alg_data[alg_data['solution_found'] == True]
            
            if not solved_data.empty:
                avg_time = solved_data['search_time'].mean()
                avg_memory = solved_data['memory_usage'].mean()
                avg_nodes = solved_data['nodes_expanded'].mean()
                avg_solution = solved_data['solution_length'].mean()
                
                # Normalize scores (0-100)
                # Coverage: tr·ª±c ti·∫øp l√† %
                coverage_score = coverage_rate
                
                # Time: inverse normalized
                max_time = self.df_solved['search_time'].max()
                time_score = (1 - avg_time / max_time) * 100 if max_time > 0 else 0
                
                # Memory: inverse normalized
                max_memory = self.df_solved['memory_usage'].max()
                memory_score = (1 - avg_memory / max_memory) * 100 if max_memory > 0 else 0
                
                # Nodes: inverse normalized
                max_nodes = self.df_solved['nodes_expanded'].max()
                nodes_score = (1 - avg_nodes / max_nodes) * 100 if max_nodes > 0 else 0
                
                # Solution quality: inverse normalized
                max_solution = self.df_solved['solution_length'].max()
                solution_score = (1 - avg_solution / max_solution) * 100 if max_solution > 0 else 0
                
                # Weighted composite score
                composite_score = (
                    coverage_score * 0.4 +  # Coverage quan tr·ªçng nh·∫•t
                    time_score * 0.25 +     # Th·ªùi gian
                    memory_score * 0.15 +   # B·ªô nh·ªõ
                    nodes_score * 0.1 +     # Nodes
                    solution_score * 0.1    # Ch·∫•t l∆∞·ª£ng l·ªùi gi·∫£i
                )
            else:
                composite_score = 0  # Kh√¥ng gi·∫£i ƒë∆∞·ª£c g√¨
            
            composite_scores[alg] = composite_score
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm
        return dict(sorted(composite_scores.items(), key=lambda x: x[1], reverse=True))

    def generate_recommendations(self, algorithm_scores):
        """T·∫°o khuy·∫øn ngh·ªã d·ª±a tr√™n k·∫øt qu·∫£ ph√¢n t√≠ch"""
        sorted_algorithms = list(algorithm_scores.keys())
        
        print("D·ª±a tr√™n k·∫øt qu·∫£ ph√¢n t√≠ch:")
        print()
        
        if len(sorted_algorithms) >= 1:
            best_alg = sorted_algorithms[0]
            print(f"üèÜ THU·∫¨T TO√ÅN T·ªêT NH·∫§T: {best_alg}")
            print(f"   - ƒêi·ªÉm t·ªïng h·ª£p cao nh·∫•t: {algorithm_scores[best_alg]:.2f}")
            print(f"   - Khuy·∫øn ngh·ªã: S·ª≠ d·ª•ng cho h·∫ßu h·∫øt c√°c b√†i to√°n")
        
        if len(sorted_algorithms) >= 2:
            second_alg = sorted_algorithms[1]
            print(f"ü•à THU·∫¨T TO√ÅN TH·ª® HAI: {second_alg}")
            print(f"   - ƒêi·ªÉm t·ªïng h·ª£p: {algorithm_scores[second_alg]:.2f}")
            print(f"   - Khuy·∫øn ngh·ªã: L·ª±a ch·ªçn thay th·∫ø t·ªët")
        
        if len(sorted_algorithms) >= 3:
            worst_alg = sorted_algorithms[-1]
            print(f"‚ö†Ô∏è  THU·∫¨T TO√ÅN C·∫¶N C·∫¢I THI·ªÜN: {worst_alg}")
            print(f"   - ƒêi·ªÉm t·ªïng h·ª£p th·∫•p nh·∫•t: {algorithm_scores[worst_alg]:.2f}")
            print(f"   - Khuy·∫øn ngh·ªã: C·∫ßn t·ªëi ∆∞u h√≥a ho·∫∑c h·∫°n ch·∫ø s·ª≠ d·ª•ng")
        
        print()
        print("üí° KHUY·∫æN NGH·ªä CHUNG:")
        print("   - ∆Øu ti√™n thu·∫≠t to√°n c√≥ coverage rate cao")
        print("   - C√¢n nh·∫Øc trade-off gi·ªØa th·ªùi gian v√† ch·∫•t l∆∞·ª£ng l·ªùi gi·∫£i")
        print("   - Ki·ªÉm tra hi·ªáu su·∫•t tr√™n c√°c lo·∫°i b√†i to√°n kh√°c nhau")
    
    def analyze_per_map_performance(self):
        """Ph√¢n t√≠ch hi·ªáu su·∫•t t·ª´ng map v√† so s√°nh v·ªõi l√Ω thuy·∫øt"""
        print("\n" + "="*80)
        print("PH√ÇN T√çCH HI·ªÜU SU·∫§T T·ª™NG MAP - SO S√ÅNH V·ªöI L√ù THUY·∫æT")
        print("="*80)
        
        maps = sorted(self.df['map'].unique())
        algorithm_theoretical_properties = {
            'BFS': {'optimal': True, 'complete': True, 'memory_efficient': False},
            'DFS': {'optimal': False, 'complete': True, 'memory_efficient': True},
            'UCS': {'optimal': True, 'complete': True, 'memory_efficient': False},
            'A*': {'optimal': True, 'complete': True, 'memory_efficient': False, 'efficient': True},
            'IDS': {'optimal': True, 'complete': True, 'memory_efficient': True}
        }
        
        total_insights = []
        
        for map_name in maps:
            print(f"\n{'='*60}")
            print(f"MAP: {map_name}")
            print(f"{'='*60}")
            
            # L·∫•y d·ªØ li·ªáu cho map n√†y
            map_data = self.df[self.df['map'] == map_name]
            map_solved = map_data[map_data['solution_found'] == True]
            
            # T·ªïng quan
            total_tested = len(map_data)
            total_solved = len(map_solved)
            success_rate = (total_solved / total_tested * 100) if total_tested > 0 else 0
            
            print(f"üìä T·ªîNG QUAN:")
            print(f"  - S·ªë thu·∫≠t to√°n test: {total_tested}")
            print(f"  - S·ªë thu·∫≠t to√°n gi·∫£i ƒë∆∞·ª£c: {total_solved}")
            print(f"  - T·ª∑ l·ªá th√†nh c√¥ng: {success_rate:.1f}%")
            
            if map_solved.empty:
                print("  ‚ö†Ô∏è  KH√îNG C√ì THU·∫¨T TO√ÅN N√ÄO T√åM ƒê∆Ø·ª¢C L·ªúI GI·∫¢I!")
                continue
            
            # Ph√¢n t√≠ch hi·ªáu su·∫•t c·ªßa t·ª´ng thu·∫≠t to√°n
            print(f"\nüìà HI·ªÜU SU·∫§T C·ª¶A T·ª™NG THU·∫¨T TO√ÅN:")
            algorithms_stats = {}
            
            for alg in sorted(map_solved['algorithm'].unique()):
                alg_data = map_solved[map_solved['algorithm'] == alg].iloc[0]
                
                stats = {
                    'search_time': alg_data['search_time'],
                    'memory_usage': alg_data['memory_usage'],
                    'nodes_expanded': alg_data['nodes_expanded'],
                    'solution_length': alg_data['solution_length']
                }
                algorithms_stats[alg] = stats
                
                print(f"  {alg}:")
                print(f"    ‚è±Ô∏è  Th·ªùi gian: {stats['search_time']:.3f}s")
                print(f"    üíæ B·ªô nh·ªõ: {stats['memory_usage']:.1f}KB")
                print(f"    üîç Nodes m·ªü r·ªông: {stats['nodes_expanded']:.0f}")
                print(f"    üìè ƒê·ªô d√†i l·ªùi gi·∫£i: {stats['solution_length']:.0f}")
            
            # So s√°nh v·ªõi l√Ω thuy·∫øt
            print(f"\nüî¨ SO S√ÅNH V·ªöI L√ù THUY·∫æT:")
            map_insights = self.analyze_theoretical_compliance(algorithms_stats, map_name)
            total_insights.extend(map_insights)
            
            # Rankings
            print(f"\nüèÜ RANKINGS:")
            
            # Ranking theo th·ªùi gian
            time_ranking = sorted(algorithms_stats.items(), key=lambda x: x[1]['search_time'])
            print(f"  ‚è±Ô∏è  Nhanh nh·∫•t:")
            for i, (alg, stats) in enumerate(time_ranking[:3], 1):
                print(f"    {i}. {alg}: {stats['search_time']:.3f}s")
            
            # Ranking theo b·ªô nh·ªõ
            memory_ranking = sorted(algorithms_stats.items(), key=lambda x: x[1]['memory_usage'])
            print(f"  üíæ Ti·∫øt ki·ªám b·ªô nh·ªõ nh·∫•t:")
            for i, (alg, stats) in enumerate(memory_ranking[:3], 1):
                print(f"    {i}. {alg}: {stats['memory_usage']:.1f}KB")
            
            # Ranking theo nodes
            nodes_ranking = sorted(algorithms_stats.items(), key=lambda x: x[1]['nodes_expanded'])
            print(f"  üîç Hi·ªáu qu·∫£ nh·∫•t (√≠t nodes):")
            for i, (alg, stats) in enumerate(nodes_ranking[:3], 1):
                print(f"    {i}. {alg}: {stats['nodes_expanded']:.0f} nodes")
            
            # Ranking theo ƒë·ªô d√†i l·ªùi gi·∫£i
            solution_ranking = sorted(algorithms_stats.items(), key=lambda x: x[1]['solution_length'])
            print(f"  üìè L·ªùi gi·∫£i t·ªëi ∆∞u nh·∫•t:")
            for i, (alg, stats) in enumerate(solution_ranking[:3], 1):
                print(f"    {i}. {alg}: {stats['solution_length']:.0f} moves")
        
        # T·ªïng k·∫øt insights
        print(f"\n" + "="*80)
        print("T·ªîNG K·∫æT INSIGHTS T·ª™ PH√ÇN T√çCH")
        print("="*80)
        
        self.summarize_insights(total_insights)
        
        return total_insights
    
    def analyze_theoretical_compliance(self, algorithms_stats, map_name):
        """Ph√¢n t√≠ch tu√¢n th·ªß l√Ω thuy·∫øt cho m·ªôt map"""
        insights = []
        
        print(f"  üîç PH√ÇN T√çCH TU√ÇN TH·ª¶ L√ù THUY·∫æT:")
        
        # 1. Ki·ªÉm tra t√≠nh t·ªëi ∆∞u
        optimal_algorithms = ['BFS', 'UCS', 'A*', 'IDS']
        optimal_present = [alg for alg in optimal_algorithms if alg in algorithms_stats]
        
        if len(optimal_present) > 1:
            solution_lengths = [algorithms_stats[alg]['solution_length'] for alg in optimal_present]
            min_length = min(solution_lengths)
            max_length = max(solution_lengths)
            
            if max_length - min_length <= 1:  # Cho ph√©p sai s·ªë nh·ªè
                print(f"    ‚úÖ T√≠nh t·ªëi ∆∞u: ƒê√öNG - C√°c thu·∫≠t to√°n t·ªëi ∆∞u ƒë·ªÅu t√¨m l·ªùi gi·∫£i ~{min_length:.0f} moves")
                insights.append(f"{map_name}: T√≠nh t·ªëi ∆∞u ƒë∆∞·ª£c ƒë·∫£m b·∫£o")
            else:
                print(f"    ‚ùå T√≠nh t·ªëi ∆∞u: SAI - S·ª± kh√°c bi·ªát l·ªõn: {min_length:.0f} - {max_length:.0f} moves")
                insights.append(f"{map_name}: C√≥ v·∫•n ƒë·ªÅ v·ªÅ t√≠nh t·ªëi ∆∞u")
        
        # 2. So s√°nh A* vs BFS (A* n√™n hi·ªáu qu·∫£ h∆°n)
        if 'A*' in algorithms_stats and 'BFS' in algorithms_stats:
            astar_nodes = algorithms_stats['A*']['nodes_expanded']
            bfs_nodes = algorithms_stats['BFS']['nodes_expanded']
            
            if astar_nodes < bfs_nodes:
                efficiency_gain = (bfs_nodes - astar_nodes) / bfs_nodes * 100
                print(f"    ‚úÖ A* vs BFS: ƒê√öNG - A* ti·∫øt ki·ªám {efficiency_gain:.1f}% nodes")
                insights.append(f"{map_name}: A* hi·ªáu qu·∫£ h∆°n BFS ({efficiency_gain:.1f}%)")
            else:
                print(f"    ‚ùå A* vs BFS: SAI - A* kh√¥ng hi·ªáu qu·∫£ h∆°n BFS")
                insights.append(f"{map_name}: A* kh√¥ng hi·ªáu qu·∫£ h∆°n BFS (c√≥ v·∫•n ƒë·ªÅ)")
        
        # 3. Ki·ªÉm tra DFS ti·∫øt ki·ªám b·ªô nh·ªõ
        if 'DFS' in algorithms_stats:
            dfs_memory = algorithms_stats['DFS']['memory_usage']
            other_memories = [stats['memory_usage'] for alg, stats in algorithms_stats.items() if alg != 'DFS']
            
            if other_memories and dfs_memory <= min(other_memories):
                print(f"    ‚úÖ DFS memory: ƒê√öNG - Ti·∫øt ki·ªám nh·∫•t ({dfs_memory:.1f}KB)")
                insights.append(f"{map_name}: DFS ti·∫øt ki·ªám b·ªô nh·ªõ nh∆∞ l√Ω thuy·∫øt")
            else:
                print(f"    ‚ùå DFS memory: SAI - Kh√¥ng ti·∫øt ki·ªám nh·∫•t")
                insights.append(f"{map_name}: DFS kh√¥ng ti·∫øt ki·ªám b·ªô nh·ªõ nh∆∞ mong ƒë·ª£i")
        
        # 4. So s√°nh UCS vs BFS (khi cost = 1, n√™n t∆∞∆°ng ƒë∆∞∆°ng)
        if 'UCS' in algorithms_stats and 'BFS' in algorithms_stats:
            ucs_solution = algorithms_stats['UCS']['solution_length']
            bfs_solution = algorithms_stats['BFS']['solution_length']
            
            if abs(ucs_solution - bfs_solution) <= 1:
                print(f"    ‚úÖ UCS vs BFS: ƒê√öNG - T∆∞∆°ng ƒë∆∞∆°ng v·ªÅ l·ªùi gi·∫£i")
                insights.append(f"{map_name}: UCS v√† BFS t∆∞∆°ng ƒë∆∞∆°ng nh∆∞ l√Ω thuy·∫øt")
            else:
                print(f"    ‚ùå UCS vs BFS: SAI - Kh√°c bi·ªát l·ªõn v·ªÅ l·ªùi gi·∫£i")
                insights.append(f"{map_name}: UCS v√† BFS kh√¥ng t∆∞∆°ng ƒë∆∞∆°ng")
        
        # 5. Ph√¢n t√≠ch th·ªùi gian ch·∫°y
        if len(algorithms_stats) > 1:
            times = [(alg, stats['search_time']) for alg, stats in algorithms_stats.items()]
            times.sort(key=lambda x: x[1])
            
            fastest = times[0]
            slowest = times[-1]
            
            print(f"    üìä Th·ªùi gian: {fastest[0]} nhanh nh·∫•t ({fastest[1]:.3f}s), {slowest[0]} ch·∫≠m nh·∫•t ({slowest[1]:.3f}s)")
            
            # Ki·ªÉm tra xem DFS c√≥ th·ªÉ nhanh h∆°n BFS trong tr∆∞·ªùng h·ª£p may m·∫Øn
            if 'DFS' in algorithms_stats and 'BFS' in algorithms_stats:
                dfs_time = algorithms_stats['DFS']['search_time']
                bfs_time = algorithms_stats['BFS']['search_time']
                
                if dfs_time < bfs_time:
                    insights.append(f"{map_name}: DFS may m·∫Øn - nhanh h∆°n BFS")
                else:
                    insights.append(f"{map_name}: BFS ·ªïn ƒë·ªãnh h∆°n DFS v·ªÅ th·ªùi gian")
        
        return insights
    
    def summarize_insights(self, insights):
        """T·ªïng k·∫øt insights t·ª´ t·∫•t c·∫£ c√°c maps"""
        print("üîç INSIGHTS T·ªîNG QU√ÅT:")
        
        # Ph√¢n lo·∫°i insights
        optimal_issues = [i for i in insights if 't·ªëi ∆∞u' in i.lower()]
        efficiency_insights = [i for i in insights if 'hi·ªáu qu·∫£' in i.lower()]
        memory_insights = [i for i in insights if 'b·ªô nh·ªõ' in i.lower()]
        stability_insights = [i for i in insights if '·ªïn ƒë·ªãnh' in i.lower() or 'may m·∫Øn' in i.lower()]
        
        if optimal_issues:
            print(f"\n  üéØ V·ªÄ T√çNH T·ªêI ∆ØU:")
            for insight in optimal_issues[:5]:  # Hi·ªÉn th·ªã top 5
                print(f"    - {insight}")
        
        if efficiency_insights:
            print(f"\n  ‚ö° V·ªÄ HI·ªÜU SU·∫§T:")
            for insight in efficiency_insights[:5]:
                print(f"    - {insight}")
        
        if memory_insights:
            print(f"\n  üíæ V·ªÄ B·ªò NH·ªö:")
            for insight in memory_insights[:5]:
                print(f"    - {insight}")
        
        if stability_insights:
            print(f"\n  üé≤ V·ªÄ T√çNH ·ªîN ƒê·ªäNH:")
            for insight in stability_insights[:5]:
                print(f"    - {insight}")
        
        # Th·ªëng k√™ t·ªïng quan
        print(f"\nüìà TH·ªêNG K√ä:")
        print(f"  - T·ªïng s·ªë insights: {len(insights)}")
        print(f"  - Insights v·ªÅ t·ªëi ∆∞u: {len(optimal_issues)}")
        print(f"  - Insights v·ªÅ hi·ªáu su·∫•t: {len(efficiency_insights)}")
        print(f"  - Insights v·ªÅ b·ªô nh·ªõ: {len(memory_insights)}")
        print(f"  - Insights v·ªÅ t√≠nh ·ªïn ƒë·ªãnh: {len(stability_insights)}")
        
        # Khuy·∫øn ngh·ªã
        print(f"\nüí° KHUY·∫æN NGH·ªä:")
        
        # D·ª±a tr√™n insights ƒë·ªÉ ƒë∆∞a ra khuy·∫øn ngh·ªã
        astar_good = len([i for i in insights if 'A*' in i and 'hi·ªáu qu·∫£' in i])
        dfs_memory_good = len([i for i in insights if 'DFS' in i and 'ti·∫øt ki·ªám b·ªô nh·ªõ' in i])
        
        if astar_good > len(insights) * 0.3:
            print("  ‚úÖ A* th·ªÉ hi·ªán hi·ªáu qu·∫£ nh∆∞ l√Ω thuy·∫øt - Khuy·∫øn ngh·ªã s·ª≠ d·ª•ng")
        
        if dfs_memory_good > len(insights) * 0.3:
            print("  ‚úÖ DFS ti·∫øt ki·ªám b·ªô nh·ªõ nh∆∞ l√Ω thuy·∫øt - T·ªët cho m√¥i tr∆∞·ªùng h·∫°n ch·∫ø")
        
        print("  üìä K·∫øt qu·∫£ cho th·∫•y c√°c thu·∫≠t to√°n ho·∫°t ƒë·ªông kh√° ph√π h·ª£p v·ªõi l√Ω thuy·∫øt")

def main():
    """Ch∆∞∆°ng tr√¨nh ch√≠nh"""
    analyzer = ResultAnalyzer()
    
    try:
        # Load d·ªØ li·ªáu
        analyzer.load_data()
        
        # Th·ª±c hi·ªán c√°c ph√¢n t√≠ch
        analyzer.analyze_success_rates()
        analyzer.analyze_performance_for_solved()
        
        # PH√ÇN T√çCH HI·ªÜU SU·∫§T T·ª™NG MAP - SO S√ÅNH V·ªöI L√ù THUY·∫æT
        analyzer.analyze_per_map_performance()
        
        # PH√ÇN T√çCH C√îNG B·∫∞NG M·ªöI
        analyzer.analyze_fair_comparison()
        
        analyzer.analyze_map_difficulty()
        analyzer.compare_algorithms_by_map()
        
        # T·∫°o bi·ªÉu ƒë·ªì
        print(f"\nƒêang t·∫°o bi·ªÉu ƒë·ªì...")
        analyzer.create_visualizations()
        
        # T·∫°o bi·ªÉu ƒë·ªì so s√°nh c√¥ng b·∫±ng
        print(f"\nƒêang t·∫°o bi·ªÉu ƒë·ªì so s√°nh c√¥ng b·∫±ng...")
        analyzer.create_fair_comparison_plots()
        
        # T·∫°o bi·ªÉu ƒë·ªì ph√¢n t√≠ch t·ª´ng map
        print(f"\nƒêang t·∫°o bi·ªÉu ƒë·ªì ph√¢n t√≠ch t·ª´ng map...")
        analyzer.create_per_map_visualization()
        
        # B√°o c√°o t·ªïng k·∫øt
        analyzer.generate_summary_report()
        
        # Bi·ªÉu ƒë·ªì ph√¢n t√≠ch t·ª´ng map
        analyzer.create_per_map_visualization()
        
        print(f"\nHo√†n th√†nh ph√¢n t√≠ch! C√°c file ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {analyzer.output_dir}")
        
    except Exception as e:
        print(f"L·ªói: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
